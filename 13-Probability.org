#+TITLE: Probability and Bayes' Theorem
#+AUTHOR: Nathan Moos
#+DATE: 2015-10-29
#+LATEX_HEADER: \newcommand*\R{\mathbb{R}}
#+LATEX_HEADER: \newcommand*\ret{\rightarrow}
#+LATEX_HEADER: \newcommand*\union{\cup}
#+LATEX_HEADER: \newcommand*\intersection{\cap}

* Bernoulli Trials

Given a success probability $p$, failure probability $q = 1-p$, the probability
of getting $k$ successes out of $n$ is.

$$ \binom{n}{k} p^k q^{n-k} $$

** Example

Weighted coin $p(H) = \frac{2}{3}$.

What is the probability of getting exactly 3 heads in 8 tosses?
$$ \binom{8}{3} (\frac{2}{3})^3 (\frac{1}{3})^5 = 6.83 \%$$

* Random Variables

*Definition:* A /random variable/ is a function from a sample space $S$ to $\R$.

Random variables are independent if:
$$ p(X = r_1 \land Y = r_2) = p(X = r_1) p(Y = r_2) \forall r_1, r_2 $$


** Example

Rolling 2 dice has the sample space $s = \{ (1, 1), (1, 2), ..., (6, 6) \}$.
You can define random variable $X : S \ret \R$ as $X(s_1, s_2) = s_1 + s_2$.

* Probability distributions

- Uniform :: all events have the same probability
- Bernoulli :: an event has two output states and (possibly) different 
               probabilities
- Geometric :: The number of times it takes to get a certain state.
               $p(n) = (1-q)^{n-1} q$

* Bayes' Theorem

Suppose that $E$ and $F$ are events from a sample space $S$ such that $p(E) > 0$
and $p(F) > 0$. 

\begin{align*}
p(F | E) &= \frac{p(F \intersection E)}{p(E)} \\
p(E | F) &= \frac{p(E \intersection F)}{p(F)} \\
\implies p(E \intersection F) &= p(F | E)p(E) \\
&= p(E | F)p(F) \\
\implies p(F | E) &= \frac{p(E | F)p(F)}{p(E)}
\end{align*}
This last expression is called /Bayes' Theorem/. The $p(F)$ factor is referred
to as the /prior/, $p(E)$ is the /normalizing factor/, $p(E|F)$ is the
/likelihood/, and $p(F|E)$ is the /posterior/.

** Useful identities

\begin{align*}
P(Y | D) + P(Y' | D) &= 1 
\end{align*}
* Expectations
  
The /expected value/ of a random variable in $S$ is the weighted average of all
values of $X$ in $S$ according to the probability distribution. This is also
referred to as the /mean/.

Expectations are /linear/.

If $X$ and $Y$ are independent random variables:
$$ E(X \cdot Y) = E(X) E(Y) $$

* Variance

The /variance/ of a random variable in $S$ is the weighted sum of the squared
difference between the value of $X(s) \quad s \in S$ and the expected value of
$X$.
* Chebyshev's Inequality

$$ p(|X(s) - E(X)| \ge r) \le \frac{\mathrm{Var}(X)}{r^2} $$
